{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras import layers\n",
    "from albumentations import (\n",
    "    Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,\n",
    "    Rotate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
    "y_train = tf.one_hot(y_train, 10)\n",
    "\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n",
    "y_test = tf.one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the data into TensorFlow `Dataset` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put aside a few samples to create our validation set\n",
    "# val samples should be \n",
    "val_samples = 2000\n",
    "x_val, y_val = x_train[:val_samples], y_train[:val_samples]\n",
    "new_x_train, new_y_train = x_train[val_samples:], y_train[val_samples:]\n",
    "\n",
    "train_ds_one = (\n",
    "    tf.data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "train_ds_two = (\n",
    "    tf.data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "# Because we will be mixing up the images and their corresponding labels, we will be\n",
    "# combining two shuffled datasets from the same training data.\n",
    "train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the mixup technique function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n",
    "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
    "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
    "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n",
    "\n",
    "\n",
    "def mix_up(ds_one, ds_two, alpha=0.2):\n",
    "    # Unpack two datasets\n",
    "    images_one, labels_one = ds_one\n",
    "    images_two, labels_two = ds_two\n",
    "    batch_size = tf.shape(images_one)[0]\n",
    "\n",
    "    # Sample lambda and reshape it to do the mixup\n",
    "    l = sample_beta_distribution(batch_size, alpha, alpha)\n",
    "    x_l = tf.reshape(l, (batch_size, 1, 1, 1))\n",
    "    y_l = tf.reshape(l, (batch_size, 1))\n",
    "\n",
    "    # Perform mixup on both images and labels by combining a pair of images/labels\n",
    "    # (one from each dataset) into one image/label\n",
    "    images = images_one * x_l + images_two * (1 - x_l)\n",
    "    labels = labels_one * y_l + labels_two * (1 - y_l)\n",
    "    return (images, labels, images_one, labels_one, images_two, labels_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the new augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28, 1) (64, 28, 28, 1)\n",
      "(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAESUlEQVR4nO3dOUsrURwF8OThBmqhoIhWVi64NVaKoIiFlVgICgpW+gVE/CRaCtpYu4CNjUXUUhEXLARRwX3FDczrQs7/PWcRx8zknF81hxnNFIebmzszSTyZTCZjQutPpk9AMksFIKcCkFMByKkA5FQAcioAORWAXI7XA+PxeJDnIQHwssanEYCcCkBOBSCnApBTAcipAORUAHIqADkVgJwKQE4FIKcCkFMByKkA5FQAcioAORWAnApATgUgpwKQUwHIqQDkPN8WLt5UVFRAnp+fh1xWVga5tbUV8vv7ezAn9gWNAORUAHIqALlIzAFqa2sh2/fNqqoqyKenp47/b29vL7V9cnIC+3p6eiDv7OxArq6uhtzW1ga5v7/f8XhraGgI8uzsrOPxP00jADkVgJwKQC7u9XsCM/l4+OrqKmT7Ph0lb29vkBsaGiAfHR392Gvp8XBxpQKQUwHIRWId4Pn52XH/1dUV5N3dXcjX19eQb25uUtt2jcGuIdjP+U9PT5Dt31vHx8eQOzs7Hff/No0A5FQAcioAuUisA+Tm5kIeHByEvLi4CPn29jawc5mZmYE8NjYG+ePjA3JTUxPk/f39YE7sP7QOIK5UAHIqALlIzAEyqbm5GfLm5ibk/Px8yCMjI5Dn5uaCOTEPNAcQVyoAORWAXCSuBfwme9/+wsICZPueb+/7t8eHnUYAcioAORWAnNYBjOnpacjj4+OQz8/PIdfX10O+u7sL5Ly+Q+sA4koFIEf/MdAu9Y6OjjoePzk5CTlMQ/53aAQgpwKQUwHI0c8BpqamINulXnv5N2pLvW40ApBTAcipAORCOQewj091dXX5+vu1tTXILS0tkEtLS798LWtrawtyXV0d5LOzM8iXl5deTzMUNAKQUwHIqQDkQnk5eHl5GXJvb++vvbZf9tF0e23g4uICcnd3N+SXl5dAzisW0+Vg8UAFIKcCkAvlHKC9vR3y0tISZHtb1sPDg+P/s7dtFRUVpbZfX19h3/b2tufzjMX+XWPIy8tzPL6kpARykPcTaA4grlQAcioAuVDOASz7uNbj4yNk+z5u2fX89K+bHx4ehn32US83lZWVkO1PxhQXF0NeX1+H/Pn56ev1/NAcQFypAORUAHKhvB/A8nuNvaCgALJ9H07n93O/Ze8HsDnsNAKQUwHIqQDkIjEH8Ku8vByy/Ur39Gv0bj8xl+00ApBTAcipAOQo5gDW/f19atv+nAwbjQDkVAByKgC5rJwDTExMOO4vLCxMbdfU1MC+g4ODQM4prDQCkFMByGXlW8DGxgbkgYEByOm3cSUSCdjX2NgIOduXijUCkFMByKkA5LJyDrCysgK5r68PckdHR2r78PAQ9tlfB892GgHIqQDkVABykXg0TL5Hj4aJKxWAnApATgUgpwKQUwHIqQDkPF8L8LhcIBGjEYCcCkBOBSCnApBTAcipAORUAHIqADkVgNxfjqsdGgeZwEoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE/UlEQVR4nO2czSs9XxzH58pjniMbWYiNh6KwpthYEDslsRAR/gK2SBIluSFPsbKyFCUPpaSEjZRCRHm2IAt+u9t8Pj/NXNy513zf79dq3s29M+fWq3M/c+ac4/n8/Pw0CCxhoW4ACS0UABwKAA4FAIcCgEMBwKEA4FAAcML9/aDH43GyHcQB/BnjYw8ADgUAhwKAQwHAoQDgUABwKAA4FAAcCgAOBQCHAoBDAcChAOBQAHAoADh+zwf4V5mbmxO5vr5e5Pb2dpG9Xq/jbQom7AHAoQDgePxdG+jmKWExMTEi9/b2+o6bmprEufj4eJEfHx9F3trasrzX3d2dyCMjIyIfHBxYfj+QcEoYsYUCgEMBwPkna4D8/HyRl5aWRM7Ozg5aW15eXkTWbbu8vHTs3qwBiC0UABwKAI4rhoJ1/ZGTkyOy/o/PzMwUOTIyUmTzf+P9/b04t7GxIfLe3p5l20pKSkSurq4WWY8rREREWF4v2LAHAIcCgEMBwHFFDZCVlSXyd8fTz87ORJ6YmPAdj42NiXNPT0/furb5Wl/x8PAg8vv7+7eu7zTsAcChAOBQAHBcUQMUFBRYntfj6aOjoyLPz8+LfH19/eO2dHR0iNzQ0GD5+ampKZGvrq5+fG8nYA8ADgUAhwKA44r5APreqampIutn6+8+y5vR7w1aWlpEHhwcFDk8XJZRut5obW0VOZjjAJwPQGyhAOBQAHBcUQM4iZ4fuLCwIHJRUZHl9w8PD0Wuq6sT+fj4+Bet+x2sAYgtFAAcCgCOK94F/JbY2FiRzesBzesEv/qsRo/td3Z2ivzX3vfbwR4AHAoADgUAJ2g1QGFhoe94f38/oNeura0VuaqqSuTy8nKR09PTf3wvPW6g3wWwBiCuggKA44rHwKioKJF3d3dFzsvLs/x+WJj0/OPjw3esp5PpLjwtLU3ksrIykfW079LSUpF3dnYs2xZq2AOAQwHAoQDguOJ1cFxcnMh667bb21uR9dKx5+dnkWdnZ33H6+vr4pze0qWyslLk/v5+kXX9oe+tt6HjNnHkT0EBwKEA4LiiBtDP8bm5uSLr//jz83PH2pKUlCTyycmJyMnJySLPzMyI3Nzc7ESzvoQ1ALGFAoBDAcBxxbsA89i9YRjG0dFRiFpiGG9vbyLrtmn0VvV/DfYA4FAAcCgAOK6oAUKJ3up1enpa5JSUFJFfX19FHhoacqZhAYI9ADgUABwKAI5jNcDAwIDI4+PjvuPT01Onbvtr9JZ0PT09ItfU1Fh+Xy81s9tuPtSwBwCHAoBDAcAJWA2QkZEhcltbm8jmMfGuri5xzs8pCQEjOjrad9zY2CjO9fX1iZyQkGB5rdXVVZGHh4d/17ggwx4AHAoADgUAJ2A1wMXFhchra2si65rAjNfrDVQzDMP4/1z8xMREkSsqKnzHunbR6HWIk5OTIi8uLoqs3wX8ddgDgEMBwKEA4Di2LkA/X+vt1YKJbrv5J+u1gSsrKyKb1xEahmHc3NwEtnEOwnUBxBYKAA4FAMexGkDPhzdv1dbd3S3OFRcXf+vaduht6DY3N0VeXl72HW9vb4tzbtvmzQrWAMQWCgCOK5aHk5/BvwBiCwUAhwKAQwHAoQDgUABwKAA4FAAcCgAOBQCHAoBDAcChAOBQAHAoADh+Lw0L9hJuEhzYA4BDAcChAOBQAHAoADgUABwKAA4FAIcCgPMfS/5cPmmWG90AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEZUlEQVR4nO2dvS88QRzG7w7x1ohEoSLRiMZLJBSESDQSiZdKQyKR8Beg0ChVKgWthI7CSyJxjVIoFCqRSDRENCTiJe7X3e+eb9jdO+Rm9nk+1T7ZPTcrn8zMzs3OJDOZTCYhaEkVuwCiuEgAciQAORKAHAlAjgQgRwKQIwHIKY16YTKZ/MtyiD8gyhifagByJAA5EoAcCUCOBCBHApAjAciRAORIAHIkADkSgBwJQI4EIEcCkCMByIk8HyBOVFVVZY+Pj4/h3OXlJeS5uTnIb29vf1ewIqAagBwJQE4y6ruBcZoSdnR0lD3u7++Hc/bfkU6nIc/MzEC+vb393cL9IpoSJkKRAORIAHIoHgPr6+sht7a2Rv7swMAA5M7OTsh3d3eQbbv78fER+buKgWoAciQAORKAnFj2AWybv7OzA7mmpubbz6ZSqcA8Pj4OeX9/v4ASuoNqAHIkADkSgJxY/hYwODgIeW9v79trX15eIO/u7kKenJyE/Pr6CrmpqQny/f095GKOA+i3ABGKBCBHApATy3GA3t5eyEH9l8XFRcinp6eQp6amIFdUVEDu6+uDvLW1FbmcLqAagBwJQI4EICcWfQDbLofN88t9Vl9fX4dz7e3tgd9l/1ZbWxtk9QGEV0gAciQAObHoAywsLEDu6uoKvH50dDR7bNv06+tryOfn55A7OjogNzY2Ri2mk6gGIEcCkCMByIlFH6ChoSHw/MrKCuSLi4vscUlJCZx7enqC/Pj4CPnz8xPyyMhI1GI6iWoAciQAORKAHC/7AHV1dZDHxsYg39zcQF5bW4P8k+2SbR/Ad1QDkCMByPGyCRgaGoJcWVkJ+erqCvLDw8OflcWn6fJfoRqAHAlAjgQgx8s+wMbGBmT7WBeWg9pte+37+ztk+xhoXwXzDdUA5EgAciQAOV72AfIln2f16upqyLW1tYHXb25uFlQmV1ANQI4EIEcCkEPRB8hnHMBOLwt7Vcx3VAOQIwHIkQDkxLIPYMfr7VJtpaX/b9vOJVhdXc3ru87OzvIrnGOoBiBHApAjAcjxog8wOzsL2S7hbtt8e96S2ydYWlqCcz09PYGfPTk5gXxwcBB4veuoBiBHApAjAcjxcrl4u4O3vYXcrWETiURieHgYcktLS/bYLuvW3NwM+fn5GbJdfsa+g+ASWi5ehCIByJEA5Dg5DmD7G3YZl7KyMsi2revu7oZ8eHgIOfdZv7y8PPBvbW9vQ3a5zS8E1QDkSAByJAA5XowD2LH9+fl5yMvLy4HXBy3rYm/fbhs3PT0N2Y4LuIzGAUQoEoAcCUCOF32A3Dl8X+WJiQnIdutYu+V7Lnap13Q6DdluFesT6gOIUCQAOV40Afa7bRNgsbdUzB28i4maABGKBCBHApDj5M/BlnyXfXNp+prrqAYgRwKQIwHI8WIcQBSGxgFEKBKAHAlAjgQgRwKQIwHIkQDkRP4t4Ce7bQp3UQ1AjgQgRwKQIwHIkQDkSAByJAA5EoAcCUDOP+U8LzOBlEPzAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEFUlEQVR4nO2dzys8YRzHd/mWkFLKwUW5ycHPckE4iBwUrkgu/AGUm5ty5ODi5EIcJCWl5aCc5OZESg4OyoVCYb+3zecpM2PbsTPzfr1O825mx7N69XmemXnm2XQ2m82mQJaSYjcAigsCiIMA4iCAOAggDgKIgwDiIIA4/4IemE6nw2wHhECQe3xUAHEQQBwEEAcBxEEAcRBAHAQQBwHEQQBxEEAcBBAHAcRBAHEQQBwEECfwfIAkUVlZmds+PT01+66urkyenZ01+f39PbyGFQEqgDgIIE466LuBSZoSlslkctu9vb1mn/vvODk5MXlmZsbk+/v7wjaugDAlDHxBAHEQQByJy8C6ujqTm5ubA3+2v7/f5JaWFpOjPAYIAhVAHAQQBwHESeQYwO3z9/f3Ta6urs773GNjYyYfHBzkfa4oQAUQBwHEQQBxEvksYGBgwOTDw8Mfj319fTV5d3fX5MnJSc/jGxoaTH58fAzczrDhWQD4ggDiIIA4ibwP0N3dbbLX+GVhYcHk8/Nzk6empkyuqKgwua+vz+SdnZ3A7YwCVABxEEAcBBAnEWOA8vJyk91n+O718Pdr9fX1dbPPnSvgdy3d3t5uMmMAiBUIIA4CiJOIMcDi4qLJnZ2dnsePjIz8uO/29tbky8tLk9va2kyur68P0sTIQgUQBwHEQQBxEjEG8OuHV1ZWTHb79e88Pz+b/PT05Hnu8fFxn9ZFGyqAOAggDgKIE8sxQG1trcmjo6Mm393dmby2tmbyx8dHOA2LIVQAcRBAnFh2AcPDwya707Sur69Nfnh4yPtvudPJ4jQ9PghUAHEQQBwEECeWY4CNjQ2T3WlbAd92C4TfuaP0Klg+UAHEQQBxEECcWI4BwqSqqsrkmpoaz+M3NzfDbE7oUAHEQQBxEEAcxgAO7pIvra2tRWrJ30AFEAcBxEEAceTHAN9/QSyVSqVWV1d/9fmLi4tCNufPoQKIgwDiIIA4sRgDzM3NmVxSYr39+voy+Tfz9paWlkzu6uryPP7s7Mxkr2Vo4wAVQBwEEAcBxInlcvGfn58mu1/h+PjY5KGhIZObmppy29vb22ZfY2OjyS8vLyZ3dHSYfHNzE6DFxYHl4sEXBBAHAcSJxX2A3+Iu33p0dGRyT09PbrusrMzsc/vNra0tk6Pc5+cDFUAcBBAHAcSJ5X2A+fl5k5eXl/M+l/u99vb2TJ6enjbZXUYuynAfAHxBAHEQQJxYjgFKS0tNnpiYMHlwcNBkr+Vc3aXjM5mMyW9vb/k0MRIwBgBfEECcWHYBEAy6APAFAcRBAHEQQBwEEAcBxEEAcRBAHAQQBwHEQQBxEEAcBBAHAcRBAHECvxpWyJ9hgehABRAHAcRBAHEQQBwEEAcBxEEAcRBAHAQQ5z+GkO8q6iEViwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADfklEQVR4nO3dvS5sURyG8TMnNBpCh4gL0FEoSGgURDRqKgmFC5BwEUoal4BGISLRiE4iNCISGlEQlajM6cR/JWaP+dhnZj/Pr/Jm9owlebP2mjXbnlK5XC7/Edbf/z0A/V8WAM4CwFkAOAsAZwHgLACcBYDrqPbAUqnUzHGoCarZ43MGgLMAcBYAzgLAWQA4CwBnAeAsAJwFgLMAcBYAzgLAWQA4CwBnAeAsAJwFgLMAcBYAzgLAWQA4CwBX9WXhVIODgyEfHByEPDo6GvLZ2VnIU1NTzRhWwzgDwFkAOAsAh18DdHZ2hry8vBzy+vp6yCMjIyF/fn6G3G63XHIGgLMAcBYADr8G6OvrC3llZSXk4eHhHEeTP2cAOAsAZwHg8GuA8fHxkMfGxup6vbW1tbqenzdnADgLAGcB4PBrgNnZ2V8d//7+HvLS0lLIDw8PdY8pT84AcBYAzgLA4dcAV1dXvzq+q6sr5HQfYX9/v+4x5ckZAM4CwFkAuFK13xtYpNvFb21tff28ubkZHuvoqLwsOj09DXl+fj7kj4+POkfXON4uXpksAJwFgCvkPkB6Hh8YGAj5+7X+Wef81M7OTsitdM6vhTMAnAWAK+QpIJ3y7+/va36tu7u7kK+vr2t+rVbkDABnAeAsAFwh1wCvr68hHx0dhVzpMrDb29uQ5+bmQq5nPdGKnAHgLACcBYAr5Bqgt7c35Ern/PQWL3t7eyEX7ZyfcgaAswBwFgCukJeE9fT0hHx4eBjyxMTE18/Pz8/hsf7+/qaNK29eEqZMFgDOAsAVch/g7e0t5IWFhZC///tW+r6fxhkAzgLAWQC4Qu4DZPn+Xj/985+envIeTtO4D6BMFgDOAsAh1wAUrgGUyQLAWQC4lvwsIL312vb2dsjd3d0Vn//y8hLyxsbGj8fe3NxUfG7ROQPAWQA4CwDXkmuAi4uLkHd3dyvm1dXVkBcXF0MeGhr68XcV/br/LM4AcBYAzgLAteQaIJV+LUsq/T+A9OtfHx8fGz2kwnAGgLMAcG3xcfD09HTIJycnFY8/Pz8PeXJysuFjagd+HKxMFgDOAsC1xdvAy8vLkI+Pj0OemZkJOb1VrH7mDABnAeAsAFxb7AOoNu4DKJMFgLMAcBYAzgLAWQA4CwBX9WcBVW4XqM04A8BZADgLAGcB4CwAnAWAswBwFgDOAsD9A2EsvE5sZ0k0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First create the new dataset using our `mix_up` utility\n",
    "train_ds_mu = train_ds.map(\n",
    "    lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.4), num_parallel_calls=AUTO\n",
    ")\n",
    "\n",
    "# Let's preview 9 samples from the dataset\n",
    "sample_images, sample_labels, images_one, labels_one, images_two, labels_two = next(iter(train_ds_mu))\n",
    "\n",
    "ds_one, ds_two = next(iter(train_ds))\n",
    "\n",
    "print(ds_one[0].shape, ds_two[0].shape)\n",
    "print(train_ds_mu.element_spec)\n",
    "\n",
    "# plot all ds_one and ds_two images\n",
    "# for i, (image, label) in enumerate(zip(ds_one[0][:6], ds_one[1][:6])):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(image.numpy().squeeze(), cmap='gray')\n",
    "#     plt.axis(\"off\")\n",
    "    # plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i, (image, label) in enumerate(zip(sample_images[:2], sample_labels[:2])):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image.numpy().squeeze(), cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # also print the original images\n",
    "    ax = plt.subplot(3, 3, i + 4)\n",
    "    plt.imshow(images_one[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    ax = plt.subplot(3, 3, i + 4)\n",
    "    plt.imshow(images_two[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "454/454 [==============================] - 7s 14ms/step - loss: 0.1778 - categorical_accuracy: 0.9498 - val_loss: 0.0888 - val_categorical_accuracy: 0.9755\n",
      "Epoch 2/10\n",
      "454/454 [==============================] - 6s 14ms/step - loss: 0.0554 - categorical_accuracy: 0.9832 - val_loss: 0.0650 - val_categorical_accuracy: 0.9810\n",
      "Epoch 3/10\n",
      "454/454 [==============================] - 6s 13ms/step - loss: 0.0389 - categorical_accuracy: 0.9879 - val_loss: 0.0562 - val_categorical_accuracy: 0.9850\n",
      "Epoch 4/10\n",
      "454/454 [==============================] - 6s 13ms/step - loss: 0.0293 - categorical_accuracy: 0.9908 - val_loss: 0.0656 - val_categorical_accuracy: 0.9840\n",
      "Epoch 5/10\n",
      "454/454 [==============================] - 6s 14ms/step - loss: 0.0226 - categorical_accuracy: 0.9930 - val_loss: 0.0530 - val_categorical_accuracy: 0.9835\n",
      "Epoch 6/10\n",
      "454/454 [==============================] - 6s 14ms/step - loss: 0.0183 - categorical_accuracy: 0.9944 - val_loss: 0.0543 - val_categorical_accuracy: 0.9865\n",
      "Epoch 7/10\n",
      "454/454 [==============================] - 6s 13ms/step - loss: 0.0132 - categorical_accuracy: 0.9957 - val_loss: 0.0629 - val_categorical_accuracy: 0.9865\n",
      "Epoch 8/10\n",
      "454/454 [==============================] - 6s 14ms/step - loss: 0.0116 - categorical_accuracy: 0.9962 - val_loss: 0.0658 - val_categorical_accuracy: 0.9865\n",
      "Epoch 9/10\n",
      "454/454 [==============================] - 6s 14ms/step - loss: 0.0095 - categorical_accuracy: 0.9968 - val_loss: 0.0700 - val_categorical_accuracy: 0.9835\n",
      "Epoch 10/10\n",
      "454/454 [==============================] - 6s 14ms/step - loss: 0.0093 - categorical_accuracy: 0.9968 - val_loss: 0.0555 - val_categorical_accuracy: 0.9875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd3ae47bcd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    layers.Conv2D(16, (5, 5), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.Conv2D(32, (5, 5), activation=\"relu\"),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "    # layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)),\n",
    "    # layers.MaxPooling2D((2, 2)),\n",
    "    # layers.Flatten(),\n",
    "    # layers.Dense(128, activation='relu'),\n",
    "    # layers.Dense(10)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds_one,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('assets/models_mu/mnist_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to combine adv dataset with train, val and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/albumentations/augmentations/transforms.py:1149: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/albumentations/augmentations/transforms.py:1175: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transforms = Compose([\n",
    "        RandomBrightness(limit=0.1),\n",
    "        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "        RandomContrast(limit=0.2, p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "    ])\n",
    "\n",
    "def aug_fn(image, img_size):\n",
    "    data = {\"image\":image}\n",
    "    aug_data = transforms(**data)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    return aug_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_adv(X_adv_train, y_adv_train, adv_mixup=False, allow_aug = False):\n",
    "    val_samples = 2000\n",
    "    x_val, y_val = x_train[:val_samples], y_train[:val_samples]\n",
    "    new_x_train, new_y_train = x_train[val_samples:], y_train[val_samples:]\n",
    "\n",
    "    if(adv_mixup):\n",
    "        # apply transforms to new_x_train\n",
    "        if(allow_aug):\n",
    "            X_adv_train = np.array([aug_fn(image, 28) for image in X_adv_train])\n",
    "\n",
    "        # create train_ds_one_adv and train_ds_two_adv\n",
    "        train_ds_one_adv = (\n",
    "            tf.data.Dataset.from_tensor_slices((X_adv_train, y_adv_train))\n",
    "            .shuffle(BATCH_SIZE * 100)\n",
    "            .batch(BATCH_SIZE)\n",
    "        )\n",
    "\n",
    "        train_ds_two_adv = (\n",
    "            tf.data.Dataset.from_tensor_slices((X_adv_train, y_adv_train))\n",
    "            .shuffle(BATCH_SIZE * 100)\n",
    "            .batch(BATCH_SIZE)\n",
    "        )\n",
    "\n",
    "        # Because we will be mixing up the images and their corresponding labels, we will be\n",
    "        # combining two shuffled datasets from the same training data.\n",
    "        train_ds_adv = tf.data.Dataset.zip((train_ds_one_adv, train_ds_two_adv))\n",
    "\n",
    "        val_ds_adv = tf.data.Dataset.from_tensor_slices(\n",
    "            (x_val, y_val)).batch(BATCH_SIZE)\n",
    "\n",
    "        test_ds_adv = tf.data.Dataset.from_tensor_slices(\n",
    "            (x_test, y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "        train_ds_mu_adv = train_ds_adv.map(\n",
    "            lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2), num_parallel_calls=AUTO\n",
    "        )\n",
    "\n",
    "        # concat X_adv_train, y_adv_train to train_ds_mu\n",
    "        train_ds = (\n",
    "            tf.data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n",
    "            .shuffle(BATCH_SIZE * 100)\n",
    "            .batch(BATCH_SIZE)\n",
    "        )\n",
    "\n",
    "        train_ds_adv = train_ds_mu_adv.concatenate(train_ds)\n",
    "\n",
    "        return train_ds_adv, val_ds_adv, test_ds_adv\n",
    "\n",
    "    else:\n",
    "\n",
    "        # apply transforms to new_x_train\n",
    "        if(allow_aug):\n",
    "            new_x_train = np.array([aug_fn(image, 28) for image in new_x_train])\n",
    "\n",
    "         # create train_ds_one and train_ds_two\n",
    "        train_ds_one = (\n",
    "            tf.data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n",
    "            .shuffle(BATCH_SIZE * 100)\n",
    "            .batch(BATCH_SIZE)\n",
    "        )\n",
    "\n",
    "        train_ds_two = (\n",
    "            tf.data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n",
    "            .shuffle(BATCH_SIZE * 100)\n",
    "            .batch(BATCH_SIZE)\n",
    "        )\n",
    "\n",
    "        # Because we will be mixing up the images and their corresponding labels, we will be\n",
    "        # combining two shuffled datasets from the same training data.\n",
    "        train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n",
    "\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE)\n",
    "\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n",
    "        \n",
    "        train_ds_mu = train_ds.map(\n",
    "            lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2), num_parallel_calls=AUTO\n",
    "        )\n",
    "\n",
    "        # concat X_adv_train, y_adv_train to train_ds_mu\n",
    "        train_ds_adv = (\n",
    "            tf.data.Dataset.from_tensor_slices((X_adv_train, y_adv_train))\n",
    "            .shuffle(BATCH_SIZE * 100)\n",
    "            .batch(BATCH_SIZE)\n",
    "        )\n",
    "        \n",
    "        train_ds_adv = train_ds_adv.concatenate(train_ds_mu)\n",
    "\n",
    "        return train_ds_adv, val_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: z3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1087/1087 - 11s - loss: 1.8601 - categorical_accuracy: 0.3320 - val_loss: 1.4989 - val_categorical_accuracy: 0.4015 - 11s/epoch - 10ms/step\n",
      "Epoch 2/15\n",
      "1087/1087 - 11s - loss: 1.4928 - categorical_accuracy: 0.3685 - val_loss: 1.2944 - val_categorical_accuracy: 0.4025 - 11s/epoch - 10ms/step\n",
      "Epoch 3/15\n",
      "1087/1087 - 11s - loss: 1.3701 - categorical_accuracy: 0.3803 - val_loss: 1.2101 - val_categorical_accuracy: 0.4285 - 11s/epoch - 10ms/step\n",
      "Epoch 4/15\n",
      "1087/1087 - 10s - loss: 0.9935 - categorical_accuracy: 0.6129 - val_loss: 0.6439 - val_categorical_accuracy: 0.6960 - 10s/epoch - 9ms/step\n",
      "Epoch 5/15\n",
      "1087/1087 - 10s - loss: 0.7599 - categorical_accuracy: 0.6541 - val_loss: 0.5379 - val_categorical_accuracy: 0.7750 - 10s/epoch - 9ms/step\n",
      "Epoch 6/15\n",
      "1087/1087 - 10s - loss: 0.5571 - categorical_accuracy: 0.8437 - val_loss: 0.2140 - val_categorical_accuracy: 0.9860 - 10s/epoch - 9ms/step\n",
      "Epoch 7/15\n",
      "1087/1087 - 10s - loss: 0.3569 - categorical_accuracy: 0.9095 - val_loss: 0.1100 - val_categorical_accuracy: 0.9885 - 10s/epoch - 9ms/step\n",
      "Epoch 8/15\n",
      "1087/1087 - 10s - loss: 0.3015 - categorical_accuracy: 0.9140 - val_loss: 0.0893 - val_categorical_accuracy: 0.9880 - 10s/epoch - 9ms/step\n",
      "Epoch 9/15\n",
      "1087/1087 - 10s - loss: 0.2760 - categorical_accuracy: 0.9169 - val_loss: 0.0623 - val_categorical_accuracy: 0.9910 - 10s/epoch - 9ms/step\n",
      "Epoch 10/15\n",
      "1087/1087 - 10s - loss: 0.2598 - categorical_accuracy: 0.9198 - val_loss: 0.0695 - val_categorical_accuracy: 0.9880 - 10s/epoch - 9ms/step\n",
      "Epoch 11/15\n",
      "1087/1087 - 10s - loss: 0.2553 - categorical_accuracy: 0.9198 - val_loss: 0.0644 - val_categorical_accuracy: 0.9895 - 10s/epoch - 9ms/step\n",
      "Epoch 12/15\n",
      "1087/1087 - 10s - loss: 0.2504 - categorical_accuracy: 0.9204 - val_loss: 0.0694 - val_categorical_accuracy: 0.9860 - 10s/epoch - 9ms/step\n",
      "Epoch 13/15\n",
      "1087/1087 - 10s - loss: 0.2441 - categorical_accuracy: 0.9216 - val_loss: 0.0721 - val_categorical_accuracy: 0.9855 - 10s/epoch - 9ms/step\n",
      "Epoch 14/15\n",
      "1087/1087 - 10s - loss: 0.2443 - categorical_accuracy: 0.9211 - val_loss: 0.0883 - val_categorical_accuracy: 0.9820 - 10s/epoch - 9ms/step\n",
      "Epoch 15/15\n",
      "1087/1087 - 10s - loss: 0.2401 - categorical_accuracy: 0.9229 - val_loss: 0.0711 - val_categorical_accuracy: 0.9865 - 10s/epoch - 9ms/step\n",
      "Training on: z6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1074/1074 - 11s - loss: 2.0668 - categorical_accuracy: 0.2425 - val_loss: 1.6281 - val_categorical_accuracy: 0.4010 - 11s/epoch - 10ms/step\n",
      "Epoch 2/15\n",
      "1074/1074 - 10s - loss: 1.4134 - categorical_accuracy: 0.4464 - val_loss: 1.1106 - val_categorical_accuracy: 0.5135 - 10s/epoch - 9ms/step\n",
      "Epoch 3/15\n",
      "1074/1074 - 10s - loss: 1.1687 - categorical_accuracy: 0.4852 - val_loss: 0.9117 - val_categorical_accuracy: 0.6170 - 10s/epoch - 10ms/step\n",
      "Epoch 4/15\n",
      "1074/1074 - 10s - loss: 0.9030 - categorical_accuracy: 0.6118 - val_loss: 0.5987 - val_categorical_accuracy: 0.7125 - 10s/epoch - 9ms/step\n",
      "Epoch 5/15\n",
      "1074/1074 - 10s - loss: 0.7110 - categorical_accuracy: 0.6863 - val_loss: 0.4537 - val_categorical_accuracy: 0.7975 - 10s/epoch - 9ms/step\n",
      "Epoch 6/15\n",
      "1074/1074 - 10s - loss: 0.5758 - categorical_accuracy: 0.7477 - val_loss: 0.3770 - val_categorical_accuracy: 0.7970 - 10s/epoch - 9ms/step\n",
      "Epoch 7/15\n",
      "1074/1074 - 11s - loss: 0.4736 - categorical_accuracy: 0.8261 - val_loss: 0.2412 - val_categorical_accuracy: 0.8995 - 11s/epoch - 10ms/step\n",
      "Epoch 8/15\n",
      "1074/1074 - 10s - loss: 0.3511 - categorical_accuracy: 0.9045 - val_loss: 0.1179 - val_categorical_accuracy: 0.9855 - 10s/epoch - 10ms/step\n",
      "Epoch 9/15\n",
      "1074/1074 - 10s - loss: 0.2782 - categorical_accuracy: 0.9213 - val_loss: 0.0850 - val_categorical_accuracy: 0.9860 - 10s/epoch - 10ms/step\n",
      "Epoch 10/15\n",
      "1074/1074 - 10s - loss: 0.2592 - categorical_accuracy: 0.9224 - val_loss: 0.0727 - val_categorical_accuracy: 0.9865 - 10s/epoch - 9ms/step\n",
      "Epoch 11/15\n",
      "1074/1074 - 10s - loss: 0.2440 - categorical_accuracy: 0.9257 - val_loss: 0.0706 - val_categorical_accuracy: 0.9860 - 10s/epoch - 9ms/step\n",
      "Epoch 12/15\n",
      "1074/1074 - 11s - loss: 0.2359 - categorical_accuracy: 0.9274 - val_loss: 0.0687 - val_categorical_accuracy: 0.9870 - 11s/epoch - 10ms/step\n",
      "Epoch 13/15\n",
      "1074/1074 - 10s - loss: 0.2288 - categorical_accuracy: 0.9293 - val_loss: 0.0540 - val_categorical_accuracy: 0.9895 - 10s/epoch - 10ms/step\n",
      "Epoch 14/15\n",
      "1074/1074 - 10s - loss: 0.2266 - categorical_accuracy: 0.9275 - val_loss: 0.0574 - val_categorical_accuracy: 0.9895 - 10s/epoch - 9ms/step\n",
      "Epoch 15/15\n",
      "1074/1074 - 10s - loss: 0.2238 - categorical_accuracy: 0.9295 - val_loss: 0.0629 - val_categorical_accuracy: 0.9875 - 10s/epoch - 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# iterate through each mnist zico defense by attack folder and train on the images inside them\n",
    "folder_path = 'assets/'\n",
    "adv_models = {}\n",
    "\n",
    "for adv_path in os.listdir(folder_path):\n",
    "    # if attack folder contains string and is a folder\n",
    "    if 'mnist_zico_defense_by_attack' in adv_path and os.path.isdir(os.path.join(folder_path, adv_path)):\n",
    "        id = adv_path.split('_')[-1]\n",
    "        if(id == 'z3' or id == 'z6'):\n",
    "            print(f'Training on: {id}')\n",
    "            # load mnist_adv dataset\n",
    "            adv_files = []\n",
    "            X = []\n",
    "            y = []\n",
    "\n",
    "            # for loop to load all the images\n",
    "            for source in range(10):\n",
    "                for target in range(10):\n",
    "                    if(source != target):\n",
    "                        file = np.load(os.path.join(folder_path, adv_path) + '/from' + str(source) + 'to' + str(target) + '.npz')\n",
    "                        adv_files.append(file)\n",
    "                        y.extend(np.full(len(file['adv_imgs']), source).tolist())\n",
    "\n",
    "            keys = adv_files[0].files\n",
    "\n",
    "            for file in adv_files:\n",
    "                with file as data:\n",
    "                    X.extend(data[keys[0]])\n",
    "\n",
    "            X = np.array(X).astype(np.float32)\n",
    "            y = np.array(y)\n",
    "            y_one_hot = np.take(np.eye(10), y, axis=0).astype(np.float32)\n",
    "\n",
    "            plt.figure(figsize=(2, 2))\n",
    "            plt.imshow(X[0], cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "            # merge X_train_adv_1 and y_train_adv_1 to create a new dataset\n",
    "            train_ds_mu_adv, val_ds_adv, test_ds_adv = merge_adv(X, y_one_hot, adv_mixup=True, allow_aug=True)\n",
    "\n",
    "            model_adv = tf.keras.models.Sequential([\n",
    "                layers.Conv2D(16, 5, activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "                layers.Conv2D(32, 5, activation=\"relu\"),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(128, activation=\"softmax\"),\n",
    "                layers.Dense(10, activation=\"softmax\"),\n",
    "            ])\n",
    "\n",
    "            model_adv.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "            )\n",
    "\n",
    "            model_adv.fit(\n",
    "                train_ds_mu_adv,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=val_ds_adv,\n",
    "                shuffle=True,\n",
    "                verbose=2\n",
    "            )\n",
    "\n",
    "            adv_models[id] = model_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adv_model in adv_models:\n",
    "    # save model_adv\n",
    "    adv_models[adv_model].save(f'assets/models_mu/mnist_adv_model_{adv_model}.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
